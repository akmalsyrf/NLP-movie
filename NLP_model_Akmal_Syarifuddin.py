# -*- coding: utf-8 -*-
"""single perceptron

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nNWIoY0y1e3SXE62AxnHS-eWdPABebSH

Nama : Akmal Syarifuddin

Dataset : Movie

Link download dataset : https://www.kaggle.com/lokkagle/movie-genre-data
"""

import pandas as pd
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
import nltk as nltk
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.layers import LSTM,Dense,Embedding,Dropout
from tensorflow.keras.models import Sequential
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

from google.colab import drive
drive.mount("/content/drive")

class ModelCallbacks(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('val_accuracy') > 0.9):
      print("Expected accuracy have been achieved")
      self.model.stop_training = True
cb = ModelCallbacks()

data = pd.read_csv('/content/drive/My Drive/kaggle_movie_train.csv', sep = ',')
data.tail()

data['genre'].value_counts()

data = data.drop(columns=['id'])

data.head()

data = data[~data['genre'].isin(['sci-fi','horror','other','adventure','romance'])]
data['genre'].value_counts()

genre = pd.get_dummies(data['genre'])
new_data = pd.concat([data, genre], axis=1)
new_data = new_data.drop(columns='genre')
new_data

new_data.columns

about_film = new_data['text'].astype(str)
genre_film = new_data[[
       'action', 'comedy', 'drama', 'thriller']].values

train_about, test_about, train_genre, test_genre = train_test_split(about_film, genre_film, test_size=0.2)

# Remove stopwords
nltk.download('stopwords')
nltk.download('punkt')

stop_words = set(stopwords.words('english'))
porter_stemmer = nltk.PorterStemmer()

# Tokenize, remove stopwords, and apply stemming
train_about_tokens = train_about.apply(lambda x: word_tokenize(x))
test_about_tokens = test_about.apply(lambda x: word_tokenize(x))

train_about_tokens = train_about_tokens.apply(lambda x: [porter_stemmer.stem(word.lower()) for word in x if word.lower() not in stop_words])
test_about_tokens = test_about_tokens.apply(lambda x: [porter_stemmer.stem(word.lower()) for word in x if word.lower() not in stop_words])

# Convert tokenized text back to string
train_about = train_about_tokens.apply(lambda x: ' '.join(x))
test_about = test_about_tokens.apply(lambda x: ' '.join(x))

# Tokenizing and padding
tokenizer = Tokenizer(num_words=5000, oov_token='*')

tokenizer.fit_on_texts(train_about)
tokenizer.fit_on_texts(test_about)

train_sequence = tokenizer.texts_to_sequences(train_about)
test_sequence = tokenizer.texts_to_sequences(test_about)

train_padded = pad_sequences(train_sequence)
test_padded = pad_sequences(test_sequence)

model = Sequential([
    Embedding(input_dim=5000, output_dim=16),
    LSTM(64),
    Dense(128, activation='relu'),
    Dropout(0.5),
    Dense(4, activation='softmax')
])
Adam(learning_rate=0.00146, name='Adam')
model.compile(optimizer = 'Adam',loss = 'categorical_crossentropy',metrics = ['accuracy'])

model.summary()

model_history = model.fit(
    train_padded,
    train_genre,
    epochs=50,
    validation_data=(test_padded, test_genre),
    verbose=2,
    batch_size=128,
    callbacks=[cb]
  )

plt.plot(model_history.history['loss'])
plt.plot(model_history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train'], loc='upper right')
plt.show()

plt.plot(model_history.history['accuracy'])
plt.plot(model_history.history['val_accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train'], loc='lower right')
plt.show()